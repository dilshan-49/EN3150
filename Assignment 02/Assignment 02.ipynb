{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69760188",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f23365",
   "metadata": {},
   "source": [
    "1. The Ordinary Least Square method is very sensitive to the outliers. As the error increases quadratically, due to the existance of outliers, the OLS fitted line deviates from the majority of the data points.We can clearly see the line is aligned towards the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b9182",
   "metadata": {},
   "source": [
    "2. Scheme 1 will be better. We need to reduce the impact of outliers on the calculation of loss. \n",
    "Therefore we need to multiply the loss caused by outliers by a scalar significantly less than what we use for inliers. Therefore using $a_i=0.01$ for outliers and $a_i = 1 $ for inliers is preffered.  \n",
    "On the other hand if we consider the scheme 2, multiplying outliers by larger value boost the impact of outliers for the loss function and makes the loss function more sensitive to the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae0c840",
   "metadata": {},
   "source": [
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde5e2b",
   "metadata": {},
   "source": [
    "4. The primary goal is to identify the regions of the brain that are predictive. The interest lies in the region, not on individual voxels. There for Group Lasso is more appropriate in this situation, where features have pre-defined group structure.  \n",
    "Standard lasso will try to identify predictive individual voxels and the result will be a sparse set of vocels scattered across all regions. So it would be difficult to conclude a region is predictive when only a handful of voxels have non-zero weights.  \n",
    "When it comes to Group Lasso it is either whole region or not. This is exactly what we need. The model will contain only the regions that are predictive as a whole. Non predictive regions are removed from the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a22fb",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d397a999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       species  class_encoded\n",
      "0       Adelie              0\n",
      "1       Adelie              0\n",
      "2       Adelie              0\n",
      "4       Adelie              0\n",
      "5       Adelie              0\n",
      "..         ...            ...\n",
      "215  Chinstrap              1\n",
      "216  Chinstrap              1\n",
      "217  Chinstrap              1\n",
      "218  Chinstrap              1\n",
      "219  Chinstrap              1\n",
      "\n",
      "[214 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the penguins dataset\n",
    "df = sns. load_dataset (\"penguins\")\n",
    "df. dropna ( inplace = True )\n",
    "\n",
    "# Filter rows for 'Adelie ' and 'Chinstrap ' classes\n",
    "selected_classes = ['Adelie', 'Chinstrap']\n",
    "df_filtered = df[df['species'].isin( selected_classes )].copy ()\n",
    "\n",
    "# Make a copy to avoid the warning\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder ()\n",
    "\n",
    "# Encode the species column\n",
    "y_encoded = le.fit_transform( df_filtered ['species'])\n",
    "df_filtered['class_encoded'] = y_encoded\n",
    "\n",
    "# Display the filtered and encoded DataFrame\n",
    "print(df_filtered [['species', 'class_encoded']])\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "y = df_filtered['class_encoded'] # Target variable\n",
    "X = df_filtered.drop(['class_encoded'], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a2010",
   "metadata": {},
   "source": [
    "2. It raised a Value Error.    \n",
    "    ```ValueError: could not convert string to float: 'Adelie'```  \n",
    "This is due to some categorical data containing string values instealof numerical values. So to resolve this we need to convert these categories into numerical values in a way that they carry a meaning.  \n",
    "We can use integer encoding but then the model might interpret in a a wrong way. For example if we use 1,2,3 for the species model might think there is some connection with the numerical value like 3 has something more compared to 1,2.  \n",
    "So the best choice is using one-hot encoding. With one-hot encoding we turn each category into a new feature, it the data point belong to that category it is 1 otherwise 0.  \n",
    "However after looking at the dataset, we have 3 categorical fields. Which are species, island and sex. Here both species and sex have only  2 classes. So we will use binary encoding. And then again the species is what we are trying to predict. So from X we will drop the species field and apply encoding for island and sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf70f411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>island_Dream</th>\n",
       "      <th>island_Torgersen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  sex  \\\n",
       "0            39.1           18.7              181.0       3750.0    1   \n",
       "1            39.5           17.4              186.0       3800.0    0   \n",
       "2            40.3           18.0              195.0       3250.0    0   \n",
       "4            36.7           19.3              193.0       3450.0    0   \n",
       "5            39.3           20.6              190.0       3650.0    1   \n",
       "\n",
       "   island_Dream  island_Torgersen  \n",
       "0         False              True  \n",
       "1         False              True  \n",
       "2         False              True  \n",
       "4         False              True  \n",
       "5         False              True  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop(['species'], axis=1, inplace=True)\n",
    "\n",
    "le_sex = LabelEncoder()\n",
    "X['sex'] = le_sex.fit_transform(X['sex'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3cfbe87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5813953488372093\n",
      "[[ 2.76753245e-03 -8.14776377e-05  4.90136556e-04 -2.88108827e-04\n",
      "   1.10826588e-05  1.85825877e-04 -1.05453230e-04]] [-8.39126135e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ML\\envs\\MIT\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train , X_test , y_train , y_test = train_test_split (X, y,test_size =0.2 , random_state =42)\n",
    "#   Train the logistic regression model . Here we are using saga\n",
    "#   solver to learn weights .\n",
    "\n",
    "logreg = LogisticRegression (solver ='saga')\n",
    "logreg.fit( X_train , y_train )\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict ( X_test )\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score (y_test , y_pred )\n",
    "print(\"Accuracy :\", accuracy )\n",
    "print(logreg .coef_ , logreg.intercept_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844cb6e",
   "metadata": {},
   "source": [
    "3. SAGA uses stochastic optimization. Since out dataset is small the stochastic nature introduce more variance and instability, leading to slightly worse results and slower convergence. Even it the warning message above it is mentioned that model failed to converge after the maximum iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b94eca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n",
      "[[ 1.5152457  -1.39159164 -0.14412318 -0.00365549 -0.22642547  0.73456302\n",
      "  -0.56189275]] [-0.07740334]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression (solver ='liblinear')\n",
    "logreg.fit( X_train , y_train )\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict ( X_test )\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score (y_test , y_pred )\n",
    "print(\"Accuracy :\", accuracy )\n",
    "print(logreg .coef_ , logreg.intercept_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685503c6",
   "metadata": {},
   "source": [
    "4. Liblinear has a classification accuracy of 1(100%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ca8fb",
   "metadata": {},
   "source": [
    "5. First of all we have a small dataset. Liblinear's deterministic approach is more stable and converge reliably. Also our dataset must have less variance.No randomness in optimization, so the results are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da88e4c",
   "metadata": {},
   "source": [
    "6. - The saga solver uses stochastic optimization, which means it updates model weights using random subsets of data. This introduces randomness in the training process. \n",
    "   - We have a small dataset. With a small dataset, the effect of randomness is amplified. Small changes in the train/test split  can lead to significant differences in which samples are used for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f25654",
   "metadata": {},
   "source": [
    "7. Let's apply Standard Scaling for bill length, bill depth, flipper length and body mass and see the performace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0824be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "X[cols] = scalar.fit_transform(X[cols])\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split (X, y,test_size =0.2 , random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57473286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n",
      "[[ 3.46414296 -0.56732773  0.36066423 -0.50455708 -1.10198737  1.39696348\n",
      "  -0.73051649]] [-2.0494785]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression (solver ='saga')\n",
    "logreg.fit( X_train , y_train )\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict ( X_test )\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score (y_test , y_pred )\n",
    "print(\"Accuracy :\", accuracy )\n",
    "print(logreg .coef_ , logreg.intercept_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69448795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n",
      "[[ 3.45311312 -0.48395813  0.3741515  -0.43662111 -1.42545299  0.88332086\n",
      "  -0.9827446 ]] [-1.29148352]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression (solver ='liblinear')\n",
    "logreg.fit( X_train , y_train )\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict ( X_test )\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score (y_test , y_pred )\n",
    "print(\"Accuracy :\", accuracy )\n",
    "print(logreg .coef_ , logreg.intercept_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e460de",
   "metadata": {},
   "source": [
    "After scaling both models have 100% accuracy. \n",
    "As a Gradient-based solver, saga is sensitive to the scale of features. If features have very different scales, the optimization can be slow or may not converge well. The Standard Scaling have minimized the high variance of data which stopped saga from converging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d96223",
   "metadata": {},
   "source": [
    "8. That approach is not correct. Whatever we do in dataprocessing must have some sort of meaning. When the using integer encoding for a feature without an ordinal relationship the value of those integers does not hold a meaning. For example if we use green-2 and red-3, here even though 3>2, it does not mean red has something more compared to green. So applying a scaler for these type of categorical data is pointless.  \n",
    "The real problem here is with the encoding method that has been used here. So what I propose is use of one hot encoding rather instead of integer encoding since this feature does not have any ordinal ralationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580eaed3",
   "metadata": {},
   "source": [
    "# Logistic Regression First/Second-Order Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b3441f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib . pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn . datasets import make_blobs\n",
    "# Generate synthetic data\n",
    "np. random . seed (0)\n",
    "centers = [[ -5 , 0], [5, 1.5]]\n",
    "X, y = make_blobs ( n_samples =2000 , centers = centers , random_state =5)\n",
    "transformation = [[0.5 , 0.5] , [ -0.5 , 1.5]]\n",
    "X = np.dot(X, transformation )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e01fa376",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b503a",
   "metadata": {},
   "source": [
    "2. The weights were initialized to zeros. For logistic regression, the loss function is convex, meaning it has a single global minimum. Initializing with zeros is a simple and computationally efficient starting point that is guaranteed to converge to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f53e1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final BGD Loss: 0.0499\n"
     ]
    }
   ],
   "source": [
    "# --- Helper Functions for Logistic Regression ---\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Computes the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(y, y_pred):\n",
    "    \"\"\"Computes the binary cross-entropy loss.\"\"\"\n",
    "    m = len(y)\n",
    "    return -1/m * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "# --- BGD Implementation ---\n",
    "\n",
    "def batch_gradient_descent(X, y, iterations=20, learning_rate=0.1):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros((n, 1)) # Initialize weights to zero\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # 1. Make predictions\n",
    "        y_pred = sigmoid(X @ weights)\n",
    "        \n",
    "        # 2. Calculate loss\n",
    "        loss = compute_loss(y, y_pred)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # 3. Calculate gradient\n",
    "        gradient = (1/m) * X.T @ (y_pred - y)\n",
    "        \n",
    "        # 4. Update weights\n",
    "        weights -= learning_rate * gradient\n",
    "        \n",
    "    return weights, loss_history\n",
    "\n",
    "# Run BGD on the first dataset\n",
    "weights, loss_history = batch_gradient_descent(X, y, iterations=20, learning_rate=0.1)\n",
    "print(f\"Final BGD Loss: {loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d2cae",
   "metadata": {},
   "source": [
    "3. For the loss function, Binary Cross Entropy aka Negative Log Likelihood has been selected. This is the standard loss function for binary classification problems. It heavily penalizes predictions that are both confident and incorrect, which effectively guides the model toward a better solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81dbf6",
   "metadata": {},
   "source": [
    "Newton's method is a second-order optimization algorithm that uses the Hessian matrix (matrix of second partial derivatives) to converge more quickly than first-order methods like gradient descent. The weight update rule is:  $$\\theta = \\theta-H^{-1} \\nabla J(\\theta)  $$ \n",
    "Where $ H $ is the Hessian Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a12c5760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Newton's Method Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# --- Newton's Method Implementation ---\n",
    "\n",
    "def newtons_method(X, y, n_iterations=20, alpha=0.5):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros((n, 1)) # Initialize weights to zero\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        # 1. Make predictions\n",
    "        y_pred = sigmoid(X @ weights)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = compute_loss(y, y_pred)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # 3. Calculate gradient\n",
    "        gradient = (1/m) * X.T @ (y_pred - y)\n",
    "        \n",
    "        # 4. Calculate Hessian\n",
    "        W = np.diag((y_pred * (1 - y_pred)).ravel())\n",
    "        H = (1/m) * X.T @ W @ X\n",
    "\n",
    "        # 5. Update weights\n",
    "        weights -= alpha*np.linalg.inv(H) @ gradient\n",
    "\n",
    "    return weights, loss_history\n",
    "\n",
    "# Run Newton's method on the first dataset\n",
    "nm_weights, nm_loss_history = newtons_method(X, y, n_iterations=20)\n",
    "print(f\"Final Newton's Method Loss: {nm_loss_history[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
