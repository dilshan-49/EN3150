\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Assignment 01}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{EN3150 - Assignment 01}\label{en3150---assignment-01}

\section{Learning from data and related challenges and linear models for
regression}\label{learning-from-data-and-related-challenges-and-linear-models-for-regression}

    \subsection{Linear Regression Impact on
outliers}\label{linear-regression-impact-on-outliers}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import}\PY{+w}{ }\PY{n+nn}{numpy}\PY{+w}{ }\PY{k}{as}\PY{+w}{ }\PY{n+nn}{np}
\PY{k+kn}{from}\PY{+w}{ }\PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model}\PY{+w}{ }\PY{k+kn}{import} \PY{n}{LinearRegression}
\PY{k+kn}{from}\PY{+w}{ }\PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics}\PY{+w}{ }\PY{k+kn}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}
\PY{k+kn}{import}\PY{+w}{ }\PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot}\PY{+w}{ }\PY{k}{as}\PY{+w}{ }\PY{n+nn}{plt}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{Finding the Linear Regression
model.}\label{finding-the-linear-regression-model.}

Here we will be using Residual Sum of Squares(RSS) to calculate the
error. So our Loss function will be,\\
\[ J(w) = \sum \limits_{i=1}^{n} (y_{i}-\hat{y}_{i})^{2} \]

Here \$ \hat{y} = Ax+B \$. After taking the derivative w.r.t. \(A\) and
\(B\) we get the following equations.

\[\frac{\partial J}{\partial A} = 2 \sum \limits_{i=1}^{n} (\hat{y} - y) x \]
\[\frac{\partial J}{\partial B} = 2 \sum \limits_{i=1}^{n} (\hat{y} - y) \]

We will use the above result to change the parameters of the model to
minimize the Lost function using iterative method.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{A} \PY{o}{=} \PY{n}{B} \PY{o}{=} \PY{l+m+mi}{0}       \PY{c+c1}{\PYZsh{} Initially setting up parameters }
\PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.001}
\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{1500}

\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{20.26}\PY{p}{,} \PY{l+m+mf}{5.61}\PY{p}{,} \PY{l+m+mf}{3.14}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{30.0} \PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{40.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{8.13}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{11.73}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{16.08}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{19.95}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{24.03}\PY{p}{]}\PY{p}{)}
\PY{n}{loss} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{param\PYZus{}A} \PY{o}{=} \PY{p}{[}\PY{n}{A}\PY{p}{,}\PY{p}{]}
\PY{n}{param\PYZus{}B} \PY{o}{=} \PY{p}{[}\PY{n}{B}\PY{p}{,}\PY{p}{]}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
    \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n}{A}\PY{o}{*}\PY{n}{x} \PY{o}{+} \PY{n}{B}
    \PY{n}{loss\PYZus{}i} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{y\PYZus{}hat} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{)}\PY{p}{)}
    \PY{n}{dA}\PY{o}{=} \PY{l+m+mi}{2}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{o}{\PYZhy{}}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{n}{x}\PY{p}{)}
    \PY{n}{dB}\PY{o}{=} \PY{l+m+mi}{2}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{o}{\PYZhy{}}\PY{n}{y}\PY{p}{)}
    \PY{n}{A}\PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr}\PY{o}{*}\PY{n}{dA}
    \PY{n}{B}\PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr}\PY{o}{*}\PY{n}{dB}
    \PY{n}{loss}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss\PYZus{}i}\PY{p}{)}
    \PY{n}{param\PYZus{}A}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{A}\PY{p}{)}
    \PY{n}{param\PYZus{}B}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{B}\PY{p}{)}

\PY{n}{x\PYZus{}model} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{9}\PY{p}{,}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{y\PYZus{}model} \PY{o}{=} \PY{n}{A}\PY{o}{*}\PY{n}{x\PYZus{}model}\PY{o}{+} \PY{n}{B}
\end{Verbatim}
\end{tcolorbox}

    \subsubsection{Plotting the model}\label{plotting-the-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{131}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{datapoints}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}model}\PY{p}{,}\PY{n}{y\PYZus{}model}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Linear model}\PY{l+s+s1}{\PYZsq{}} \PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear Model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{132}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{loss}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{param\PYZus{}A}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gradient}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{param\PYZus{}B}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{intercept}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mi}{1500}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficients}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Assignment 01_files/Assignment 01_6_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Loss Calculation}\label{loss-calculation}

    \begin{itemize}
\tightlist
\item
  Now let's consider the new loss function to reduce the effect of
  outliers.
  \[ L(\theta , \beta) = \frac{1}{N} \sum \limits_{i=1}^{N} \frac{(y_i - \hat{y}_i)^{2}}{(y_i - \hat{y}_i)^2 + \beta^2} \]
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def}\PY{+w}{ }\PY{n+nf}{calc\PYZus{}loss}\PY{p}{(}\PY{n}{A}\PY{p}{,}\PY{n}{B}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{beta}\PY{p}{,}\PY{n}{N}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
    \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n}{A}\PY{o}{*}\PY{n}{x} \PY{o}{+} \PY{n}{B}

    \PY{n}{error} \PY{o}{=} \PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}hat}
    \PY{n}{denom} \PY{o}{=} \PY{n}{error}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{n}{beta}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}

    \PY{n}{loss} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{N}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{error}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{/} \PY{n}{denom}\PY{p}{)}

    \PY{k}{return} \PY{n}{loss}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{N} \PY{o}{=} \PY{l+m+mi}{10}

\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{20.26}\PY{p}{,} \PY{l+m+mf}{5.61}\PY{p}{,} \PY{l+m+mf}{3.14}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{30.0} \PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{40.0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{8.13}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{11.73}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{16.08}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{19.95}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{24.03}\PY{p}{]}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Loss for model 1}
\PY{n}{A1} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{4}
\PY{n}{B1} \PY{o}{=} \PY{l+m+mi}{12} 
\PY{n}{M1\PYZus{}loss} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0} \PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}

\PY{n}{M1\PYZus{}loss}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{calc\PYZus{}loss}\PY{p}{(}\PY{n}{A1}\PY{p}{,}\PY{n}{B1}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{l+m+mf}{0.000001}\PY{p}{,}\PY{n}{N}\PY{p}{)}    \PY{c+c1}{\PYZsh{}Loss for beta=10\PYZca{}\PYZhy{}6}
\PY{n}{M1\PYZus{}loss}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{calc\PYZus{}loss}\PY{p}{(}\PY{n}{A1}\PY{p}{,}\PY{n}{B1}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{N}\PY{p}{)}           \PY{c+c1}{\PYZsh{}Loss for beta=1}
\PY{n}{M1\PYZus{}loss}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{calc\PYZus{}loss}\PY{p}{(}\PY{n}{A1}\PY{p}{,}\PY{n}{B1}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{,}\PY{n}{N}\PY{p}{)}        \PY{c+c1}{\PYZsh{}Loss for beta=1000}


\PY{c+c1}{\PYZsh{} Loss for model 2}
\PY{n}{A2} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{3.55}
\PY{n}{B2} \PY{o}{=} \PY{l+m+mf}{3.91} 
\PY{n}{M2\PYZus{}loss} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0} \PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}

\PY{n}{M2\PYZus{}loss}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{calc\PYZus{}loss}\PY{p}{(}\PY{n}{A2}\PY{p}{,}\PY{n}{B2}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{l+m+mf}{0.000001}\PY{p}{,}\PY{n}{N}\PY{p}{)}    \PY{c+c1}{\PYZsh{}Loss for beta=10\PYZca{}\PYZhy{}6}
\PY{n}{M2\PYZus{}loss}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{n}{calc\PYZus{}loss}\PY{p}{(}\PY{n}{A2}\PY{p}{,}\PY{n}{B2}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{N}\PY{p}{)}           \PY{c+c1}{\PYZsh{}Loss for beta=1}
\PY{n}{M2\PYZus{}loss}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{calc\PYZus{}loss}\PY{p}{(}\PY{n}{A2}\PY{p}{,}\PY{n}{B2}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{,}\PY{n}{N}\PY{p}{)}        \PY{c+c1}{\PYZsh{}Loss for beta=1000}



\PY{n}{beta\PYZus{}vals} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{1e\PYZhy{}6} \PY{p}{,} \PY{l+m+mi}{1} \PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{]}

\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{beta\PYZus{}vals}\PY{p}{,} \PY{n}{M1\PYZus{}loss}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{beta\PYZus{}vals}\PY{p}{,} \PY{n}{M2\PYZus{}loss}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xscale}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss vs beta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Assignment 01_files/Assignment 01_10_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For beta = 1 Model 1 Loss: 0.435416262490386\\
For beta = 10\^{}-6 Model 1 Loss: 0.9999999998258207\\
For beta = 10\^{}3 Model 1 Loss: 0.0002268287498440988\\
For beta = 1 Model 2 Loss: 0.9728470518681676\\
For beta = 10\^{}-6 Model 2 Loss: 0.9999999999999718\\
For beta = 10\^{}3 Model 2 Loss: 0.00018824684654645654

    \begin{itemize}
\item
  \textbf{For very small values of \(\beta\) (\(\beta = 10^{-3}\)) :}
  \textgreater The loss values for both models are extremely close to 1
  (e.g., 0.9999\ldots). This happens because the denominator term,
  \((y_i​−\hat{y}_​i​)^2+β^2\), is dominated by the residual squared for
  almost every data point. The robust loss function essentially treats
  all points as large outliers and caps the loss for each point at a
  value of 1. As a result, the total loss becomes approximately 1,
  regardless of the model's performance. This value of β makes the loss
  function insensitive to changes in the model, making it useless for
  optimization.
\item
  \textbf{For very large values of \(\beta\) (\(\beta = 10^6\)) :}\\
  \textgreater With a large \(\beta\), the denominator term is dominated
  by \(\beta^2\). The loss function for each data point simplifies to
  approximately \(\frac{(y_i​−\hat{y}_​i​)^2}{\beta^2}​\), which is just a
  scaled version of the standard squared error. In this scenario, the
  robust nature of the loss function is lost. Outliers with large
  residuals will still have a large, dominant influence on the total
  loss, which is precisely what the robust estimator is designed to
  prevent.
\item
  \textbf{For balanced value of \(\beta\) (\(\beta = 1\)) :}
  \textgreater This intermediate value allows the loss function to
  function as intended. For inliers (points with small residuals), the
  loss function behaves like a squared error, rewarding a good fit. For
  outliers (points with large residuals), the loss is capped at a value
  close to 1, preventing them from skewing the model.
\end{itemize}

    The most suitable value is \(\beta=1\). When comparing the loss of
\emph{model 1} and \emph{model 2} we can see significant difference when
\(\beta=1\). Which indicates one model had been siginificantly affected
by outliers.

    \subsubsection{Selecting the most suitable
model}\label{selecting-the-most-suitable-model}

\begin{itemize}
\item
  Partial Derivatives of the loss function w.r.t. out model parameters
  \(A\) and \(B\) are as follows,
  \[  \frac{\partial L}{\partial A} = \frac{2\beta^2}{N} \sum \limits_{i=1}^{N} \frac{(y_i - \hat{y}) x}{((y_i - \hat{y}_i)^2 + \beta^2)^2} \]
  \[  \frac{\partial L}{\partial B} = \frac{2\beta^2}{N}  \sum \limits_{i=1}^{N} \frac{(y_i - \hat{y}) }{((y_i - \hat{y}_i)^2 + \beta^2)^2} \]
\item
  We will use the above functions to calculate the gradient and tune our
  parameters.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Setting parameters with the values between both models}
\PY{n}{A} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mf}{3.55}
\PY{n}{B} \PY{o}{=} \PY{l+m+mf}{3.91}    

\PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.005}

\PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{50000}
\PY{n}{beta}\PY{o}{=}\PY{l+m+mi}{1}

\PY{n}{loss} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{param\PYZus{}A} \PY{o}{=} \PY{p}{[}\PY{n}{A}\PY{p}{,}\PY{p}{]}
\PY{n}{param\PYZus{}B} \PY{o}{=} \PY{p}{[}\PY{n}{B}\PY{p}{,}\PY{p}{]}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
    \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n}{A}\PY{o}{*}\PY{n}{x} \PY{o}{+} \PY{n}{B}

    \PY{n}{error} \PY{o}{=} \PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}hat}
    \PY{n}{denom} \PY{o}{=} \PY{n}{error}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{n}{beta}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}

    \PY{n}{loss\PYZus{}i} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{N}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{error}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{/} \PY{n}{denom}\PY{p}{)}

    \PY{n}{dA}\PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{N}\PY{p}{)}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{n}{beta}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{n}{error}\PY{o}{*}\PY{n}{x}\PY{o}{/}\PY{p}{(}\PY{n}{denom}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
    \PY{n}{dB}\PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{N}\PY{p}{)}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{n}{beta}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{n}{error}\PY{o}{/}\PY{p}{(}\PY{n}{denom}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
    
    \PY{n}{A}\PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr}\PY{o}{*}\PY{n}{dA}
    \PY{n}{B}\PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr}\PY{o}{*}\PY{n}{dB}
    
    \PY{n}{loss}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss\PYZus{}i}\PY{p}{)}
    \PY{n}{param\PYZus{}A}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{A}\PY{p}{)}
    \PY{n}{param\PYZus{}B}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{B}\PY{p}{)}

\PY{n}{x\PYZus{}model} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{9}\PY{p}{,}\PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{y\PYZus{}model} \PY{o}{=} \PY{n}{A}\PY{o}{*}\PY{n}{x\PYZus{}model}\PY{o}{+} \PY{n}{B}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{131}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{datapoints}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}model}\PY{p}{,}\PY{n}{y\PYZus{}model}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Linear model}\PY{l+s+s1}{\PYZsq{}} \PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear Model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{132}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{loss}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{param\PYZus{}A}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gradient}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{param\PYZus{}B}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{intercept}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficients}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Assignment 01_files/Assignment 01_16_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Here we can see parameters are closer to the parameters of model 1.
Therefore model 1 is more suitable.

    \subsubsection{Mechanism of the robust
estimator.}\label{mechanism-of-the-robust-estimator.}

The robust estimator mitigates the influence of outliers by imposing an
upper bound on the loss contributed by any single data point. The
behavior of the loss function, defined for a single point as
\(L_i​=\frac{r_i^2​}{r_i^2​+β^2}\) where \(r_i\)\hspace{0pt} is the
residual, is determined by the magnitude of the residual relative to the
hyperparameter \(β\).

For inliers, where the residual is small compared to \(β (r_i​≪β)\), the
loss function approximates a scaled quadratic error,
\(L_i​≈ \frac{r_i^2}{β^2}\). In this regime, its behavior is analogous to
the standard Mean Squared Error (MSE) objective, thereby fitting the
model closely to these points.

Conversely, for outliers, where the residual is large \((r_i​≫β)\), the
loss function saturates and approaches a constant value of 1. Unlike
MSE, where an outlier's contribution to the loss can grow without bound,
this property ensures that the influence of outliers is effectively
capped.

This saturation prevents the model parameters from being
disproportionately skewed by a few extreme values, resulting in a
regression that is more robust and representative of the underlying
structure of the inlier data.

    \subsubsection{Alternative Loss
Functions}\label{alternative-loss-functions}

Alternative robust loss function is the Huber Loss.

It treats errors differently based on their magnitude. It defines a
threshold hyperparameter, delta (δ).

\begin{itemize}
\tightlist
\item
  For small errors (inliers), it uses the squared error.
\item
  For large errors (outliers), it switches to the absolute error.
\end{itemize}

This creates a piecewise function defined as: \[
L_{\delta}(r) =
\begin{cases}
  \frac{1}{2}r^2 & \text{for } |r| \le \delta \\
  \delta\left(|r| - \frac{1}{2}\delta\right) & \text{for } |r| > \delta
\end{cases}
\]

Where: - r is the residual \((y−\hat{y}​\)). - δ is the threshold.

    \subsection{Loss Function}\label{loss-function}

    \subsubsection{Calculating the loss}\label{calculating-the-loss}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def}\PY{+w}{ }\PY{n+nf}{calc\PYZus{}MSE}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}hat}\PY{p}{)}\PY{p}{:}
    \PY{n}{N}\PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}

    \PY{n}{MSE} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}hat}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{N}

    \PY{k}{return} \PY{n}{MSE}

\PY{k}{def}\PY{+w}{ }\PY{n+nf}{calc\PYZus{}BCE}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}hat}\PY{p}{)}\PY{p}{:}
    \PY{n}{N} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}
    \PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{clip}\PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}15}\PY{p}{,} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{l+m+mf}{1e\PYZhy{}15}\PY{p}{)} \PY{c+c1}{\PYZsh{}Avoid Zero Division Warnings}

    \PY{n}{BCE} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{N}\PY{p}{)} \PY{o}{*} \PY{p}{(}\PY{n}{y} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{y\PYZus{}hat}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{y}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{y\PYZus{}hat}\PY{p}{)}\PY{p}{)}

    \PY{k}{return} \PY{n}{BCE}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{y\PYZus{}true} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{13}\PY{p}{,}\PY{p}{)}\PY{p}{)}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.005}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.05}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{0.4}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.6}\PY{p}{,} \PY{l+m+mf}{0.7}\PY{p}{,} \PY{l+m+mf}{0.8}\PY{p}{,} \PY{l+m+mf}{0.9}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{)}

\PY{n}{MSE} \PY{o}{=} \PY{n}{calc\PYZus{}MSE}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
\PY{n}{BCE} \PY{o}{=} \PY{n}{calc\PYZus{}BCE}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from}\PY{+w}{ }\PY{n+nn}{tabulate}\PY{+w}{ }\PY{k+kn}{import} \PY{n}{tabulate}

\PY{c+c1}{\PYZsh{} Create a DataFrame to display the values}
\PY{n}{data} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y\PYZus{}true}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{list}\PY{p}{(}\PY{n}{y\PYZus{}true}\PY{p}{)}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y\PYZus{}pred}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{list}\PY{p}{(}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{list}\PY{p}{(}\PY{n}{MSE}\PY{p}{)}\PY{p}{,}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BCE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n+nb}{list}\PY{p}{(}\PY{n}{BCE}\PY{p}{)}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Convert to tabulate format}
\PY{n}{table} \PY{o}{=} \PY{n}{tabulate}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{headers}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{keys}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{tablefmt}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{grid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Print the table}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{table}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{MSE :}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{MSE}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BCE :}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{BCE}\PY{p}{)}\PY{p}{)}

\PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{MSE}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{BCE}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BCE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE vs BCE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
+----------+----------+-------------+-------------+
|   y\_true |   y\_pred |         MSE |         BCE |
+==========+==========+=============+=============+
|        1 |    0.005 | 0.0761558   | 0.407563    |
+----------+----------+-------------+-------------+
|        1 |    0.01  | 0.0753923   | 0.354244    |
+----------+----------+-------------+-------------+
|        1 |    0.05  | 0.0694231   | 0.230441    |
+----------+----------+-------------+-------------+
|        1 |    0.1   | 0.0623077   | 0.177122    |
+----------+----------+-------------+-------------+
|        1 |    0.2   | 0.0492308   | 0.123803    |
+----------+----------+-------------+-------------+
|        1 |    0.3   | 0.0376923   | 0.0926133   |
+----------+----------+-------------+-------------+
|        1 |    0.4   | 0.0276923   | 0.0704839   |
+----------+----------+-------------+-------------+
|        1 |    0.5   | 0.0192308   | 0.053319    |
+----------+----------+-------------+-------------+
|        1 |    0.6   | 0.0123077   | 0.0392943   |
+----------+----------+-------------+-------------+
|        1 |    0.7   | 0.00692308  | 0.0274365   |
+----------+----------+-------------+-------------+
|        1 |    0.8   | 0.00307692  | 0.0171649   |
+----------+----------+-------------+-------------+
|        1 |    0.9   | 0.000769231 | 0.00810466  |
+----------+----------+-------------+-------------+
|        1 |    1     | 0           | 7.68616e-17 |
+----------+----------+-------------+-------------+


MSE : 0.440201923076923
BCE : 1.601589090996543
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Assignment 01_files/Assignment 01_24_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Selection of Loss
Function}\label{selection-of-loss-function}

\begin{itemize}
\item
  For the Application 01 where the output is a continous variable Linear
  regression is used. We need to estimate a line which moves closer to
  our datapoints. Therefore the Mean Squared error is suitable. In MSE
  when prediction deviates heavily, due to squaring the error Loss
  function return larger values leading to faster convergance.
\item
  In Binary classification model output is a probaility. BCE measures
  the difference of model output probability and actual binary label.
  With the logarithmic penalization of BCE, the incorrect predictions
  with high confidence are heavily penalized. Also BCE provides
  gradients that are well-suited for optimizing models that output
  probabilities, such as logistic regression. Therefore BCE is suitable
  as the loss function for the Application 02.
\end{itemize}

    \subsection{Data Pre-Processing}\label{data-pre-processing}

    \subsubsection{Generating the Features}\label{generating-the-features}

Provided code is used to generate features

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import}\PY{+w}{ }\PY{n+nn}{numpy}\PY{+w}{ }\PY{k}{as}\PY{+w}{ }\PY{n+nn}{np}
\PY{k+kn}{import}\PY{+w}{ }\PY{n+nn}{matplotlib} \PY{o}{.} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}

\PY{k}{def}\PY{+w}{ }\PY{n+nf}{generate\PYZus{}signal} \PY{p}{(} \PY{n}{signal\PYZus{}length} \PY{p}{,} \PY{n}{num\PYZus{}nonzero} \PY{p}{)}\PY{p}{:}
    \PY{n}{signal} \PY{o}{=} \PY{n}{np}\PY{o}{.} \PY{n}{zeros} \PY{p}{(} \PY{n}{signal\PYZus{}length} \PY{p}{)}
    \PY{n}{nonzero\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.} \PY{n}{random} \PY{o}{.} \PY{n}{choice} \PY{p}{(} \PY{n}{signal\PYZus{}length} \PY{p}{,} \PY{n}{num\PYZus{}nonzero} \PY{p}{,}
    \PY{n}{replace} \PY{o}{=} \PY{k+kc}{False} \PY{p}{)}
    \PY{n}{nonzero\PYZus{}values} \PY{o}{=} \PY{l+m+mi}{10}\PY{o}{*} \PY{n}{np}\PY{o}{.} \PY{n}{random} \PY{o}{.} \PY{n}{randn} \PY{p}{(} \PY{n}{num\PYZus{}nonzero} \PY{p}{)}
    \PY{n}{signal} \PY{p}{[} \PY{n}{nonzero\PYZus{}indices} \PY{p}{]} \PY{o}{=} \PY{n}{nonzero\PYZus{}values}
    \PY{k}{return} \PY{n}{signal}

\PY{n}{signal\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{100} \PY{c+c1}{\PYZsh{} Total length of the signal}
\PY{n}{num\PYZus{}nonzero} \PY{o}{=} \PY{l+m+mi}{10} \PY{c+c1}{\PYZsh{} Number of non \PYZhy{} zero elements in thesignal}
\PY{n}{your\PYZus{}index\PYZus{}no} \PY{o}{=} \PY{l+m+mi}{220067} 
\PY{n}{sparse\PYZus{}signal} \PY{o}{=} \PY{n}{generate\PYZus{}signal} \PY{p}{(} \PY{n}{signal\PYZus{}length} \PY{p}{,} \PY{n}{num\PYZus{}nonzero} \PY{p}{)}
\PY{n}{sparse\PYZus{}signal} \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]} \PY{o}{=} \PY{p}{(} \PY{n}{your\PYZus{}index\PYZus{}no} \PY{o}{\PYZpc{}} \PY{l+m+mi}{10}\PY{p}{)} \PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{l+m+mi}{10}
\PY{k}{if} \PY{n}{your\PYZus{}index\PYZus{}no} \PY{o}{\PYZpc{}} \PY{l+m+mi}{10} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
    \PY{n}{sparse\PYZus{}signal} \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.} \PY{n}{random} \PY{o}{.} \PY{n}{randn} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{30}
    \PY{n}{sparse\PYZus{}signal} \PY{o}{=} \PY{n}{sparse\PYZus{}signal} \PY{o}{/}\PY{l+m+mi}{5}

\PY{n}{epsilon} \PY{o}{=} \PY{n}{np}\PY{o}{.} \PY{n}{random} \PY{o}{.} \PY{n}{normal} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}

\PY{c+c1}{\PYZsh{} epsilon = epsilon [:, np. newaxis ]}
\PY{n}{plt}\PY{o}{.} \PY{n}{figure} \PY{p}{(} \PY{n}{figsize} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{15} \PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{subplot} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xlim} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ Feature 1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{18}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xticks} \PY{p}{(} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{18}\PY{p}{)} \PY{c+c1}{\PYZsh{} Adjust x\PYZhy{} axis tick label font size}
\PY{n}{plt}\PY{o}{.} \PY{n}{yticks} \PY{p}{(} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{18}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{stem} \PY{p}{(} \PY{n}{sparse\PYZus{}signal} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{subplot} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xlim} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ Feature 2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{18}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{stem} \PY{p}{(} \PY{n}{epsilon} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xticks} \PY{p}{(} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{18}\PY{p}{)} \PY{c+c1}{\PYZsh{} Adjust x\PYZhy{} axis tick label font size}
\PY{n}{plt}\PY{o}{.} \PY{n}{yticks} \PY{p}{(} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{18}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{show} \PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Assignment 01_files/Assignment 01_28_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Scaling the features}\label{scaling-the-features}

We will use standard scaling, min-max scaling, and max-abs scaling from
sklearn.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from}\PY{+w}{ }\PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing}\PY{+w}{ }\PY{k+kn}{import} \PY{n}{StandardScaler}\PY{p}{,} \PY{n}{MinMaxScaler}\PY{p}{,} \PY{n}{MaxAbsScaler}

\PY{n}{standard} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{min\PYZus{}max} \PY{o}{=} \PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{max\PYZus{}abs} \PY{o}{=} \PY{n}{MaxAbsScaler}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{itemize}
\item
  \textbf{Standard Scaling:} Transforms data to have a mean of 0 and a
  standard deviation of 1. It achieves this by subtracting the mean from
  each data point (centering) and then dividing by the standard
  deviation.
\item
  \textbf{Min-Max Scaling:} Rescales the data to a fixed range,
  typically {[}0, 1{]}. It works by shifting and rescaling the values
  based on the minimum and maximum values present in the feature.
\item
  \textbf{Max-Abs Scaling:} Divides every data point by the maximum
  absolute value in the feature set. This scales the data to the range
  {[}-1, 1{]} and is a non-centering method.
\end{itemize}

    \paragraph{Scaling the feature 1(Sparse
Signal)}\label{scaling-the-feature-1sparse-signal}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{standard\PYZus{}feature1} \PY{o}{=} \PY{n}{standard}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{sparse\PYZus{}signal}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{minmax\PYZus{}feature1} \PY{o}{=} \PY{n}{min\PYZus{}max}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{sparse\PYZus{}signal}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{maxabs\PYZus{}feature1} \PY{o}{=} \PY{n}{max\PYZus{}abs}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{sparse\PYZus{}signal}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}


\PY{n}{plt}\PY{o}{.} \PY{n}{figure} \PY{p}{(} \PY{n}{figsize} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{15} \PY{p}{,}\PY{l+m+mi}{16}\PY{p}{)} \PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature 1 \PYZhy{} Sparse Signal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}

\PY{n}{plt}\PY{o}{.} \PY{n}{subplot} \PY{p}{(}\PY{l+m+mi}{411}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xlim} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Initial Signal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{stem} \PY{p}{(} \PY{n}{sparse\PYZus{}signal} \PY{p}{)}

\PY{n}{plt}\PY{o}{.} \PY{n}{subplot} \PY{p}{(}\PY{l+m+mi}{412}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xlim} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ Standard Scaling}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{stem} \PY{p}{(} \PY{n}{standard\PYZus{}feature1} \PY{p}{)}

\PY{n}{plt}\PY{o}{.} \PY{n}{subplot} \PY{p}{(}\PY{l+m+mi}{413}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xlim} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ Min\PYZhy{}Max Scaling}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{stem} \PY{p}{(} \PY{n}{minmax\PYZus{}feature1} \PY{p}{)}

\PY{n}{plt}\PY{o}{.} \PY{n}{subplot} \PY{p}{(}\PY{l+m+mi}{414}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xlim} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ Max\PYZhy{}Abs Scaling}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{stem} \PY{p}{(} \PY{n}{maxabs\PYZus{}feature1} \PY{p}{)}


\PY{n}{plt}\PY{o}{.} \PY{n}{show} \PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Assignment 01_files/Assignment 01_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For the sparse signal, \emph{Max-Abs Scaling} is the most suitable
method.

\textbf{Justification:} The primary characteristic of this feature is
its sparsity---a large proportion of its values are exactly zero.
Preserving this structure is critical. Max-Abs Scaling is the only
method presented that guarantees the preservation of zeros, as the
operation \((0/max(∣xi​∣)=0)\) does not alter them. It successfully
scales the non-zero values while maintaining the integrity of the
feature's sparse nature. In contrast, both Standard Scaling and Min-Max
Scaling involve shifting the data, which converts the original zero
entries into non-zero values, thereby destroying the feature's
fundamental sparse structure.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{standard\PYZus{}feature2} \PY{o}{=} \PY{n}{standard}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{epsilon}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{minmax\PYZus{}feature2} \PY{o}{=} \PY{n}{min\PYZus{}max}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{epsilon}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{maxabs\PYZus{}feature2} \PY{o}{=} \PY{n}{max\PYZus{}abs}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{epsilon}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}


\PY{n}{plt}\PY{o}{.} \PY{n}{figure} \PY{p}{(} \PY{n}{figsize} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{15} \PY{p}{,}\PY{l+m+mi}{12}\PY{p}{)} \PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature 2 \PYZhy{} Epsilon Signal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=} \PY{l+m+mi}{16}\PY{p}{)}

\PY{n}{plt}\PY{o}{.} \PY{n}{subplot} \PY{p}{(}\PY{l+m+mi}{411}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xlim} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Initial Signal}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{stem} \PY{p}{(} \PY{n}{epsilon} \PY{p}{)}

\PY{n}{plt}\PY{o}{.} \PY{n}{subplot} \PY{p}{(}\PY{l+m+mi}{412}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xlim} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ Standard Scaling}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{stem} \PY{p}{(} \PY{n}{standard\PYZus{}feature2} \PY{p}{)}

\PY{n}{plt}\PY{o}{.} \PY{n}{subplot} \PY{p}{(}\PY{l+m+mi}{413}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xlim} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ Min\PYZhy{}Max Scaling}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{stem} \PY{p}{(} \PY{n}{minmax\PYZus{}feature2} \PY{p}{)}

\PY{n}{plt}\PY{o}{.} \PY{n}{subplot} \PY{p}{(}\PY{l+m+mi}{414}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{xlim} \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{signal\PYZus{}length} \PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ Max\PYZhy{}Abs Scaling}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fontsize} \PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
\PY{n}{plt}\PY{o}{.} \PY{n}{stem} \PY{p}{(} \PY{n}{maxabs\PYZus{}feature2} \PY{p}{)}


\PY{n}{plt}\PY{o}{.} \PY{n}{show} \PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Assignment 01_files/Assignment 01_35_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For the dense, epsilon-like signal, \emph{Standard Scaling} is the most
suitable method.

\textbf{Justification:} This feature is dense and appears to be
symmetrically distributed around a mean of zero, which is characteristic
of statistical noise. The key property to preserve is this
distributional shape. Standard Scaling is ideal for this task because it
standardizes the feature to have a mean of 0 and a unit variance. While
Min-Max Scaling normalizes the range, it undesirably shifts the data
away from its natural zero center. Max-Abs Scaling, though it preserves
the center, is less statistically robust for dense data as it relies on
a single extreme value for scaling rather than the overall
distribution's standard deviation.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
